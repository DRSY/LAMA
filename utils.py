'''
Author: roy
Date: 2020-10-30 22:18:56
LastEditTime: 2020-10-31 11:24:32
LastEditors: Please set LastEditors
Description: In User Settings Edit
FilePath: /LAMA/utils.py
'''
import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from torch.distributions import Bernoulli

device = torch.device("cuda:3")
print(device)

class FoobarPruning(prune.BasePruningMethod):
    """
    Customized Pruning Method
    """
    PRUNING_TYPE = 'unstructured'

    def __init__(self, pregenerated_mask) -> None:
        super().__init__()
        self.pre_generated_mask = pregenerated_mask

    def compute_mask(self, t, default_mask):
        """
        """
        mask = self.pre_generated_mask.clone()
        return mask


def Foobar_pruning(module, name):
    """
    util function for pruning parameters of given module.name using corresponding mask generated by relation-specific mask generator
    Parameters:
    module: subclass of nn.Module
    name: name of parameters to be pruned
    id: id for the parameters in the parameters_tobe_pruned list
    """
    sub_module = getattr(module, name)
    shape = sub_module.size()
    random_mask = torch.bernoulli(torch.empty(
        *shape).uniform_(0, 1)).float().to(device)
    FoobarPruning.apply(module, name, pregenerated_mask=random_mask)
    return module

def remove_prune_reparametrization(module, name):
    prune.remove(module, name)


def bernoulli_sampler(probs):
    Bernoulli_Sampler = Bernoulli(probs=probs)
    sample = Bernoulli_Sampler.sample()
    log_probs_of_sample = Bernoulli_Sampler.log_prob(sample)
    return sample, log_probs_of_sample

if __name__ == "__main__":
    model = nn.Sequential(nn.Linear(32,100), nn.ReLU(), nn.Linear(100, 200), nn.Sigmoid())
    model.train()
    inputs = torch.randn(16, 32)
    pruning_mask_probs = model(inputs)
    assert pruning_mask_probs.requires_grad == True
    samples, log_probs_of_samples = bernoulli_sampler(probs=pruning_mask_probs)
    print(log_probs_of_samples)